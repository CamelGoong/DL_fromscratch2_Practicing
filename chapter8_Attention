# 가중합(맥락벡터 c) 구하기
import numpy as np

T, H = 5, 4
hs = np.random.randn(T, H)
a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])

ar = a. reshape(5,1).repeat(4, axis = 1)
print(ar.shape)

t = hs * ar
print(t.shape)


c = np.sum(t, axis = 0)
print(c.shape)

# 미니배치 처리용 가중합(맥락벡터 c) 구하기
N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
a = np.random.randn(N, T)
ar = a.reshape(N, T, 1).repeat(H, axis = 2)

t = hs * ar
print(t.shape)

c = np.sum(t, axis = 1)
print(c.shape)

class WeightSum:
  def __init__(self):
    self.params, self.grads = [], []
    self.cache = None
  
  def forward(self, hs, a):
    N, T, H = hs.shape

    ar = a.reshape(N, T, 1).repeat(H, axis = 2)
    t = hs * ar
    c = np.sum(t, axis = 1)
    self.cache = (hs, ar)

    return c

  def backward(self, dc):
    hs, ar = self.cache
    N, T, H = hs.shape

    dt = dc.reshape(N, 1, H).repeat(T, axis = 1) # sum의 역전파인 repeat 노드
    dar = dt * hs # 곱의 역전파는 원래 반대편거 곱하는 거
    dhs = dt * ar
    da = np.sum(ar, axis = 2) # repeat의 역전파는 sum

    return dhs, da
# AttentionWeight 계층 구현
import numpy as np
# from common.layers import Softmax
class Softmax:
    def __init__(self):
        self.params, self.grads = [], []
        self.out = None

    def forward(self, x):
        self.out = softmax(x)
        return self.out

    def backward(self, dout):
        dx = self.out * dout
        sumdx = np.sum(dx, axis=1, keepdims=True)
        dx -= self.out * sumdx
        return dx

# 본격적인 AttentionWeight 계층 구현
class AttentionWeight:
  def __init__(self):
    self.params, self.grads = [], []
    self.softmax = Softmax()
    self.cache = None

  def forward(self, hs, h):
    N, T, H = hs.shape

    hr = h.reshape(N, 1, H).repeat(T, axis = 1)
    t = hs * hr
    s = np.sum(t, axis = 2)
    a = self.softmax.forward(s)

    self.cache = (hs, hr)
    return a

  def backward(self, da):
    hs, hr = self.cache
    N, T, H = hs.shape

    ds = self.softmax.backward(da)
    dt = ds.reshape(N, T, 1).repeat(H, axis = 2)
    dhs = dt * hr
    hr = dt * hs
    dh = np.sum(dhr, axis = 1)

    return dhs, dh

    
